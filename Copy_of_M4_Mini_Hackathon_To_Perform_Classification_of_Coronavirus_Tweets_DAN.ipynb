{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prachitshukla/Team-2/blob/coronavirus_sentiment_analysis/Copy_of_M4_Mini_Hackathon_To_Perform_Classification_of_Coronavirus_Tweets_DAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "\n",
        "### Mini Project Notebook: To perform text classification of coronavirus tweets during the peak Covid - 19 period using LSTMs/RNNs/CNNs/BERT.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maritime-miami"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nljJR6CwfZN_"
      },
      "source": [
        "At the end of the mini-hackathon, you will be able to :\n",
        "\n",
        "* perform data preprocessing/preprocess the text\n",
        "* represent the text/words using the pretrained word embeddings - Word2Vec/Glove\n",
        "* build the deep neural network (RNN, LSTM, GRU, CNNs, Bidirectional-LSTM, GRU, BERT) to classify the tweets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "First we need to understand why sentiment analysis is needed for social media?\n",
        "\n",
        "People from all around the world have been using social media more than ever. Sentiment analysis on social media data helps to understand the wider public opinion about certain topics such as movies, events, politics, sports, and more and gain valuable insights from this social data. Sentiment analysis has some powerful applications. Nowadays it is also used by some businesses to do market research and understand the customer’s experiences for their products or services.\n",
        "\n",
        "Now an interesting question about this type of problem statement that may arise in your mind is that why sentiment analysis on COVID-19 Tweets? What is about the coronavirus tweets that would be positive? You may have heard sentiment analysis on movie or book reviews, but what is the purpose of exploring and analyzing this type of data?\n",
        "\n",
        "The use of social media for communication during the time of crisis has increased remarkably over the recent years. As mentioned above, analyzing social media data is important as it helps understand public sentiment. During the coronavirus pandemic, many people took to social media to express their anger, grief, or sadness while some also spread happiness and positivity. People also used social media to ask their network for help related to vaccines or hospitals during this hard time. Many issues related to this pandemic can also be solved if experts considered this social data. That’s the reason why analyzing this type of data is important to understand the overall issues faced by people.\n",
        "\n"
      ],
      "metadata": {
        "id": "iNI_-0spy1Ho"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL0Ve1abn6YJ"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The given challenge is to build a multiclass classification model to predict the sentiment of Covid-19 tweets. The tweets have been pulled from Twitter and manual tagging has been done. We are given information like Location, Tweet At, Original Tweet, and Sentiment.\n",
        "\n",
        "The training dataset consists of 36000 tweets and the testing dataset consists of 8955 tweets. There are 5 sentiments namely ‘Positive’, ‘Extremely Positive’, ‘Negative’, ‘Extremely Negative’, and ‘Neutral’ in the sentiment column.\n",
        "\n",
        "## Description\n",
        "\n",
        "This dataset has the following information about the user who tweeted:\n",
        "\n",
        "1. **UserName:** twitter handler\n",
        "2. **ScreenName:** a personal identifier on Twitter and is separate from the username\n",
        "3. **Location:** where in the world the person tweets from\n",
        "4. **TweetAt:** date of the tweet posted (DD-MM-YYYY)\n",
        "5. **OriginalTweet:** the tweet itself\n",
        "6. **Sentiment:** sentiment value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWGmjNHdZul"
      },
      "source": [
        "To build and implement a multiclass classification deep neural network model to classify between Positive/Extremely Positive/Negative/Extremely Negative/Neutral sentiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BQEA97zTlTa"
      },
      "source": [
        "## Grading = 10 Marks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdYmy-tJgURN"
      },
      "source": [
        "Here is a handy link to Kaggle's competition documentation (https://www.kaggle.com/docs/competitions), which includes, among other things, instructions on submitting predictions (https://www.kaggle.com/docs/competitions#making-a-submission)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8OapRtHgLnU"
      },
      "source": [
        "## Instructions for downloading train and test dataset from Kaggle API are as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO2jS73oLnCR"
      },
      "source": [
        "### 1. Create an API key in Kaggle.\n",
        "\n",
        "To do this, go to the competition site on Kaggle at (https://www.kaggle.com/t/db0ea322e4b14ad1b29d14fbe406d4e5) and open your user settings page. Click Account.\n",
        "\n",
        "* Click on your profile picture at the top-right corner of the page.\n",
        "\n",
        "![alt text](https://i.imgur.com/kSLmEj2.png)\n",
        "\n",
        "* In the popout menu, click the Settings option.\n",
        "\n",
        "![alt text](https://i.imgur.com/tNi6yun.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkzGffHdbwX2"
      },
      "source": [
        "### 2. Next, scroll down to the API access section and click generate to download an API key (kaggle.json).\n",
        "![alt text](https://i.imgur.com/vRNBgrF.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtETuXx8b-OC"
      },
      "source": [
        "### 3. Upload your kaggle.json file using the following snippet in a code cell:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1pfXBDxWl0Y"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCV_T6MMW4eX"
      },
      "source": [
        "#If successfully uploaded in the above step, the 'ls' command here should display the kaggle.json file.\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbukdzJ6cE32"
      },
      "source": [
        "### 4. Install the Kaggle API using the following command\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMj1n1MJcqzN"
      },
      "source": [
        "#!pip uninstall urllib3\n",
        "#!pip install urllib3>=1.26.11\n",
        "!pip install -U -q kaggle==1.5.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1 List of installed pakage"
      ],
      "metadata": {
        "id": "VyKd4-O27sAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "OH4JiGGr74Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vpy9P1nchhd"
      },
      "source": [
        "### 5. Move the kaggle.json file into ~/.kaggle, which is where the API client expects your token to be located:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQbPsDOLZ0b4"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BenAWlpI73sm"
      },
      "source": [
        "# Execute the following command to verify whether the kaggle.json is stored in the appropriate location: ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm2jGsCradOS"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json # run this command to ensure your Kaggle API token is secure on colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32unPZKzdI72"
      },
      "source": [
        "### 6. Now download the Test Data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppuy5gRKHFwv"
      },
      "source": [
        "**NOTE: If you get a '404 - Not Found' error after running the cell below, it is most likely that the user (whose kaggle.json is uploaded above) has not 'accepted' the rules of the competition and therefore has 'not joined' the competition.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41-ETZCE_A1j"
      },
      "source": [
        "If you encounter **401-unauthorised** download latest **kaggle.json** by repeating steps 1 & 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If you get a forbidden link, you have most likely not joined the competition.\n",
        "!kaggle competitions download -c perform-classification-of-coronavirus-tweets"
      ],
      "metadata": {
        "id": "BYJqsfpFk5n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/perform-classification-of-coronavirus-tweets.zip"
      ],
      "metadata": {
        "id": "jZRrRMaz90Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOUR CODING STARTS FROM HERE"
      ],
      "metadata": {
        "id": "QeKon2vruI_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* install gensim"
      ],
      "metadata": {
        "id": "TxnpyI-4_Acp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abstract-stocks"
      },
      "source": [
        "## Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import chardet\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import itertools\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot as plt\n",
        "#import matplotlib\n",
        "#import matplotlib.patches as mpatches\n",
        "tsne = TSNE(n_components=2)\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "O5RcxwQUku6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53g0zVbjRV7K"
      },
      "source": [
        "##   **Stage 1**:  Data Loading and Perform Exploratory Data Analysis (1 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Load the Dataset\n"
      ],
      "metadata": {
        "id": "xIa9LlhMHj5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the positive and negative files and split the sentences into a list\n",
        "with open('corona_nlp_test.csv/corona_nlp_test.csv',\"rb\") as data_test:\n",
        "  result = chardet.detect(data_test.read())\n",
        "  print(result)\n",
        "  data_test_set = pd.read_csv('corona_nlp_test.csv/corona_nlp_test.csv', encoding=result['encoding'])\n",
        "\n",
        "with open('corona_nlp_train.csv/corona_nlp_train.csv',\"rb\") as data_train:\n",
        "  result = chardet.detect(data_train.read())\n",
        "  print(result)\n",
        "  data_train_set = pd.read_csv('corona_nlp_train.csv/corona_nlp_train.csv', encoding=result['encoding'])"
      ],
      "metadata": {
        "id": "6qmR5Vo_tbVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* check first 5 records of train dataframe"
      ],
      "metadata": {
        "id": "rUI14yJ8NFZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train_set.head())"
      ],
      "metadata": {
        "id": "t7YiYySENJ7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Check for Missing Values"
      ],
      "metadata": {
        "id": "hzQS91rfJLNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train_set.isnull().sum())"
      ],
      "metadata": {
        "id": "JF3xCD9qJN1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize the sentiment column values\n"
      ],
      "metadata": {
        "id": "nra2K6EPHosw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train_set[\"Sentiment\"])"
      ],
      "metadata": {
        "id": "5ksnP-I2Fitd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize top 10 Countries that had the highest tweets using countplot (Tweet count vs Location)\n"
      ],
      "metadata": {
        "id": "_zc6AUq9Hry8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "sns.countplot(data=data_train_set, x=data_train_set['Location'],  order= data_train_set['Location'].value_counts().iloc[:10].index)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M_fUPMJzGl8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Plotting Pie Chart for the Sentiments in percentage\n"
      ],
      "metadata": {
        "id": "GUIM_P-VHuzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "sentiment_count={}\n",
        "for sentiment in data_train_set['Sentiment'].unique():\n",
        "  sentiment_count[sentiment]=data_train_set['Sentiment'].value_counts()[sentiment]\n",
        "  print(sentiment,data_train_set['Sentiment'].value_counts()[sentiment])\n",
        "plt.pie(sentiment_count.values(), labels=sentiment_count.keys(), autopct='%1.1f%%')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s8oRZOYDHAVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* WordCloud for the Tweets/Text\n",
        "\n",
        "    * Visualize the most commonly used words in each sentiment using wordcloud\n",
        "    * Refer to the following [link](https://medium.com/analytics-vidhya/word-cloud-a-text-visualization-tool-fb7348fbf502) for Word Cloud: A Text Visualization tool\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cSvzz5z6H8kM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "text=' '.join(data_train_set['OriginalTweet'].astype(str))\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vq7__byEHabv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oLyIb19KcdL"
      },
      "source": [
        "##   **Stage 2**: Data Pre-Processing  (2 Points)\n",
        "\n",
        "####  Clean and Transform the data into a specified format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* function to preprocess the data"
      ],
      "metadata": {
        "id": "rIwuBr1nxJP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing function\n",
        "def preprocess_text(sen):\n",
        "\n",
        "    sen = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", sen)\n",
        "    sen = re.sub(r\"\\'s\", \" \\'s\", sen)\n",
        "    sen = re.sub(r\"[\\([{})\\]]\", \"\", sen)\n",
        "\n",
        "    # Tokenizing words\n",
        "    tokens = word_tokenize(sen)\n",
        "\n",
        "    # Converting to lower case\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "\n",
        "     # Remove punctuations\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "\n",
        "    # Remove non alphabet\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Remove stop words\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "\n",
        "    return words# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "-6ZCiIxxKiq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* list of unique sentiments"
      ],
      "metadata": {
        "id": "eEiNUBeTxOUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of sentiments\n",
        "print(data_train_set['Sentiment'].unique())"
      ],
      "metadata": {
        "id": "zKXJQyMtsdtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* function to segrigate sentiment specific words"
      ],
      "metadata": {
        "id": "xaBjfYQ6kKpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def words_per_sentiment(sentences_per_sentiment):\n",
        "  lines_for_sentiments = []\n",
        "  # segrigate sentiment specific words\n",
        "  for sen in sentences_per_sentiment:\n",
        "      # Call the preprocess_text function on each sentence of the review text\n",
        "      lines_for_sentiments.append(preprocess_text(sen))\n",
        "  return lines_for_sentiments\n"
      ],
      "metadata": {
        "id": "dD-CqATVkP2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* segrigate sentiment specific words"
      ],
      "metadata": {
        "id": "32YnGfgdxQ-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the preprocessed reviews in a new list- positive\n",
        "sentences_pos = list(data_train_set.loc[data_train_set['Sentiment']=='Positive', 'OriginalTweet'])\n",
        "print(len(sentences_pos))\n",
        "\n",
        "lines_pos = words_per_sentiment(sentences_pos)\n",
        "print(len(lines_pos))\n",
        "print(lines_pos[0])\n"
      ],
      "metadata": {
        "id": "euaFuVtmptly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the preprocessed reviews in a new list- Extremely Positive\n",
        "sentences_ext_pos = list(data_train_set.loc[data_train_set['Sentiment']=='Extremely Positive', 'OriginalTweet'])\n",
        "print(len(sentences_ext_pos))\n",
        "\n",
        "lines_ext_pos = words_per_sentiment(sentences_ext_pos)\n",
        "print(len(lines_ext_pos))\n",
        "print(lines_ext_pos[0])"
      ],
      "metadata": {
        "id": "knp-k0N2sphr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the preprocessed reviews in a new list- Neutral\n",
        "sentences_neu = list(data_train_set.loc[data_train_set['Sentiment']=='Neutral', 'OriginalTweet'])\n",
        "print(len(sentences_neu))\n",
        "\n",
        "lines_neu = words_per_sentiment(sentences_neu)\n",
        "print(len(lines_neu))\n",
        "print(lines_neu[0])"
      ],
      "metadata": {
        "id": "GJBtsKHOspQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the preprocessed reviews in a new list- Negative\n",
        "sentences_neg = list(data_train_set.loc[data_train_set['Sentiment']=='Negative', 'OriginalTweet'])\n",
        "print(len(sentences_neg))\n",
        "\n",
        "lines_neg = words_per_sentiment(sentences_neg)\n",
        "print(len(lines_neg))\n",
        "print(lines_neg[0])"
      ],
      "metadata": {
        "id": "GKtqRauIso-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the preprocessed reviews in a new list- Extremely Negative\n",
        "sentences_ext_neg = list(data_train_set.loc[data_train_set['Sentiment']=='Extremely Negative', 'OriginalTweet'])\n",
        "print(len(sentences_ext_neg))\n",
        "\n",
        "lines_ext_neg = words_per_sentiment(sentences_ext_neg)\n",
        "print(len(lines_ext_neg))\n",
        "print(lines_ext_neg[0])"
      ],
      "metadata": {
        "id": "KDm0QPqjsokB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* convert sentiment specific list of words to simple word list"
      ],
      "metadata": {
        "id": "qD3HdXiaw-lM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_ext_pos = list(itertools.chain.from_iterable(lines_ext_pos))\n",
        "text_pos = list(itertools.chain.from_iterable(lines_pos))\n",
        "text_neu = list(itertools.chain.from_iterable(lines_neu))\n",
        "text_neg = list(itertools.chain.from_iterable(lines_neg))\n",
        "text_ext_neg = list(itertools.chain.from_iterable(lines_ext_neg))\n",
        "\n",
        "print(f'''Extremely Positive : {len(text_ext_pos)} \\t Positive : {len(text_pos)} \\t Neutral : {len(text_neu)} \\t Negative : {len(text_neg)} \\t Extremely Negative : {len(text_ext_neg)}''')\n"
      ],
      "metadata": {
        "id": "hSdbPomZxG3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZg7yL2TtTM"
      },
      "source": [
        "##   **Stage 3**: Build the Word Embeddings using pretrained Word2vec/Glove (Text Representation) (1 Point)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Import GloVe Embedding Files"
      ],
      "metadata": {
        "id": "MFay7W9y66jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "ipython = get_ipython()\n",
        "ipython.magic(\"sx wget -qq https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/glove.6B.zip\")\n",
        "ipython.magic(\"sx unzip glove.6B.zip\")"
      ],
      "metadata": {
        "id": "EMATcPo26_uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* create GloVe 50d embedding"
      ],
      "metadata": {
        "id": "IYMAFkZf5W0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GloVe_Dict_50d = {}\n",
        "# Loading the 50-dimensional vector of the model\n",
        "with open(\"glove.6B.50d.txt\", 'r') as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      GloVe_Dict_50d[word] = vector\n",
        "\n",
        "print(len(GloVe_Dict_50d))"
      ],
      "metadata": {
        "id": "_9rTpuiwSy0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Vector representation of words"
      ],
      "metadata": {
        "id": "Zcq2XGSkTN0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_vectors(text):\n",
        "  vectors = []\n",
        "  for word in text:\n",
        "    try:\n",
        "      vector = GloVe_Dict_50d[word]\n",
        "      vectors.append(vector)\n",
        "    except KeyError:\n",
        "      pass\n",
        "  print(\"There are %d words and the vector size of each word is %d\" %((len(vectors),len(vectors[0]))))\n",
        "  return vectors"
      ],
      "metadata": {
        "id": "u29RhlGmTRwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Passing the words present in text_neg and text_pos to the function gen_vectors\n",
        "vectors_ext_pos = gen_vectors(text_ext_pos)\n",
        "vectors_pos = gen_vectors(text_pos)\n",
        "vectors_neu = gen_vectors(text_neu)\n",
        "vectors_neg = gen_vectors(text_neg)\n",
        "vectors_ext_neg = gen_vectors(text_ext_neg)"
      ],
      "metadata": {
        "id": "dUs434fzTWA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Find cosine similarity"
      ],
      "metadata": {
        "id": "Ft4YISZFUHTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_cosine_similarity(text):\n",
        "  word_similarity = []\n",
        "  index = []\n",
        "  for i, word_1 in enumerate(text):\n",
        "    row_wise_simiarity = []\n",
        "    print(i,word_1)\n",
        "    if(i == 4):\n",
        "      break\n",
        "    for j, word_2 in enumerate(text):\n",
        "      # Get the vectors of the word using GloVe\n",
        "      try:\n",
        "        vec_1, vec_2 = GloVe_Dict_50d[word_1], GloVe_Dict_50d[word_2]\n",
        "      except KeyError:\n",
        "        pass\n",
        "\n",
        "      # As the vectors are in one dimensional, convert it to 2D by reshaping\n",
        "      vec_1, vec_2 = np.array(vec_1).reshape(1,-1), np.array(vec_2).reshape(1,-1)\n",
        "\n",
        "      # Measure the cosine similarity between the vectors.\n",
        "      similarity = cosine_similarity(vec_1, vec_2)\n",
        "      row_wise_simiarity.append(np.array(similarity).item())\n",
        "\n",
        "    # Store the cosine similarity values in a list\n",
        "    word_similarity.append(row_wise_simiarity)\n",
        "    index.append(word_1)\n",
        "\n",
        "  # Create a DataFrame to view the similarity between words\n",
        "  return pd.DataFrame(word_similarity, columns=text, index = index)"
      ],
      "metadata": {
        "id": "0C1ynf5mUJmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_neu = find_cosine_similarity(text_neu)\n",
        "df_ext_pos = find_cosine_similarity(text_ext_pos)\n",
        "df_pos = find_cosine_similarity(text_pos)\n",
        "df_neg = find_cosine_similarity(text_neg)\n",
        "df_ext_neg = find_cosine_similarity(text_ext_neg)"
      ],
      "metadata": {
        "id": "Kuolz6OsUbV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def glove_embeddings(text, dim):\n",
        "    if len(text) < 1:\n",
        "        return np.zeros(dim)\n",
        "    else:\n",
        "        vectorized = [GloVe_Dict_50d[word] if word in GloVe_Dict_50d else np.random.randn(dim) for word in text]\n",
        "    sum = np.sum(vectorized, axis=0)\n",
        "    # Return the average vector\n",
        "    return sum/len(vectorized)\n",
        "\n",
        "def get_glove_embeddings(text, dimension):\n",
        "        embeddings = text.apply(lambda x: glove_embeddings(x, dimension))\n",
        "        return list(embeddings)"
      ],
      "metadata": {
        "id": "WcppY4OLYXTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualization of word vectors using TSNE"
      ],
      "metadata": {
        "id": "fQ4MXiJD8AY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = get_glove_embeddings(data_train_set['OriginalTweet'], dimension=50)"
      ],
      "metadata": {
        "id": "sw_NHGA88MeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tsne_visualization(word_embeddings):\n",
        "    x = word_embeddings[1:100]\n",
        "    x = np.asarray(x)\n",
        "    y = tsne.fit_transform(x)\n",
        "    plt.figure(figsize=(20,10))\n",
        "    colors=['orange','red']\n",
        "    sns.scatterplot(x=y[:,0],y=y[:,1],hue=data_train_set['Sentiment'].iloc[1:100])\n",
        "\n",
        "    for label,x,y in zip(data_train_set['Sentiment'].iloc[1:100],y[:, 0],y[:,1]):\n",
        "        plt.annotate(label,xy=(x,y),xytext=(0,0),textcoords='offset points')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FsxpAoHh8zQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne_visualization(word_embeddings)"
      ],
      "metadata": {
        "id": "UhLjtFdt80Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "data_train_set['OriginalTweet'] = data_train_set['OriginalTweet'].apply(lambda x:simple_preprocess(x, max_len=30))\n",
        "\n",
        "# Remove stop words\n",
        "data_train_set['OriginalTweet'] = data_train_set['OriginalTweet'].apply(lambda x: [w for w in x if not w in stop_words])\n",
        "\n",
        "data_train_set.head()"
      ],
      "metadata": {
        "id": "4MZhkamwDeBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Replace Sentiment values to number"
      ],
      "metadata": {
        "id": "3r25njp8F7lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_set[\"Sentiment\"] = data_train_set[\"Sentiment\"].apply(lambda x:4 if x == \"Extremely Positive\" else (3 if x == \" Positive\" else(2 if x == \"Neutral\" else(1 if x == \"Negative\" else(0)))))\n",
        "data_train_set.head()"
      ],
      "metadata": {
        "id": "8ltjIEWrF53E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store OriginalTweet and Sentiment\n",
        "X = data_train_set[\"OriginalTweet\"]\n",
        "y = data_train_set['Sentiment']"
      ],
      "metadata": {
        "id": "5gEHC9-6GEGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get GloVe embedding for OriginalTweet\n",
        "train_embeddings  = get_glove_embeddings(data_train_set['OriginalTweet'], dimension=50)\n",
        "print(len(train_embeddings))"
      ],
      "metadata": {
        "id": "amo1ZIf4GE0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Prepaire Train and Test Sets"
      ],
      "metadata": {
        "id": "0_5mUr5XGH2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the train_embeddings in X\n",
        "X = np.array(train_embeddings)\n",
        "\n",
        "# Converting X into torch tensor\n",
        "X = torch.Tensor(X)\n",
        "\n",
        "# Reshaping X to 200 dimension\n",
        "X = X.reshape(-1, 50)\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "bLAJa5JRGKFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the labels in y\n",
        "y = data_train_set['Sentiment']\n",
        "\n",
        "# Converting X into torch tensor\n",
        "y = torch.Tensor(y)\n",
        "\n",
        "# Reshaping y to 1 dimension\n",
        "y = y.reshape(-1,1)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "_EO3ljbQGN-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Prepaire Train and Validation set"
      ],
      "metadata": {
        "id": "U_udASeFGRbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, train_size = 30000)"
      ],
      "metadata": {
        "id": "Q74j_-hAGR5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up device to run CUDA operations\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "uMJrhLvHGUJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(X_train,y_train)\n",
        "train_loader = DataLoader(train_dataset,batch_size = 32)\n",
        "test_dataset = TensorDataset(X_test,y_test)\n",
        "test_loader = DataLoader(test_dataset,batch_size = 32)"
      ],
      "metadata": {
        "id": "rTkR-AF1GWwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6jfm3YFUL7i"
      },
      "source": [
        "##   **Stage 4**: Build and Train the Deep Recurrent Model using Pytorch/Keras (4 Points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DAN(torch.nn.Module):\n",
        "        def __init__(self, input_size, hidden_size, dp = 0.5, d_out = 5):\n",
        "            super(DAN, self).__init__()\n",
        "            self.input_size = input_size\n",
        "            self.hidden_size  = hidden_size\n",
        "            self.bn1 = nn.BatchNorm1d(input_size)\n",
        "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "            self.dropout1 = nn.Dropout(dp)\n",
        "            self.bn2 = nn.BatchNorm1d(self.hidden_size)\n",
        "            self.fc2 = torch.nn.Linear(self.hidden_size, 10)\n",
        "            self.fc3 = torch.nn.Linear(10, d_out)\n",
        "\n",
        "        def forward(self, x):\n",
        "            # x = self.dropout1(x)\n",
        "            x = self.bn1(x)\n",
        "            x = self.fc1(x)\n",
        "            x = self.dropout1(x)\n",
        "            x = self.bn2(x)\n",
        "            x = self.fc2(x)\n",
        "            x = self.fc3(x)\n",
        "            return x"
      ],
      "metadata": {
        "id": "dK5xbGxXGZZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimension as 50 and layers as 32\n",
        "model = DAN(50, 32)\n",
        "#model = DAN(50, 64)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# We will set the learning rate (lr) as 0.001\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "ea5uuChRGhgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Training Model"
      ],
      "metadata": {
        "id": "KxW4QGvwGpqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First switch the module mode to model.train() so that new weights can be learned after every epoch.\n",
        "model.train()\n",
        "\n",
        "# No of Epochs\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  # Iterate through all the batches in each epoch\n",
        "  for inputs, target in train_loader:\n",
        "\n",
        "    # Zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Compute Loss\n",
        "    target = target.squeeze_()\n",
        "    target = target.type(torch.LongTensor)\n",
        "\n",
        "    loss = criterion(outputs, target)\n",
        "\n",
        "  print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
        "\n",
        "  # Backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # optimizer.step() updates the weights accordingly\n",
        "  optimizer.step()\n",
        "\n",
        "print(\"We got the training loss as %f for %d epochs\" %((loss.item(), epochs)))"
      ],
      "metadata": {
        "id": "gVM8nOeCGjVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-O0Jx99UhmI"
      },
      "source": [
        "##   **Stage 5**: Evaluate the Model and get model predictions on the test dataset (2 Points)\n",
        "\n",
        "* Upload the model predictions to kaggle by mapping the sentiment column vlalues from numericals the categorical\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating empty lists to store the labels and the predictions\n",
        "labels = []\n",
        "predictions = []"
      ],
      "metadata": {
        "id": "PSaAlhGGUitF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "for inputs,target in test_loader:\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    _,out = torch.max(outputs, 1)\n",
        "\n",
        "    labels.append(out)\n",
        "\n",
        "    target = target.squeeze_()\n",
        "    target = target.type(torch.LongTensor)\n",
        "\n",
        "    predictions.append(target)\n",
        "    loss = criterion(outputs,target)\n",
        "\n",
        "print(\"We got the loss as %f for test set.\" %((loss.item())))"
      ],
      "metadata": {
        "id": "V69O1x2_J6ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check Label and Prediction\n",
        "labels = torch.cat(labels, 0)\n",
        "predictions = torch.cat(predictions,0)"
      ],
      "metadata": {
        "id": "1ArM4rqxJ_g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j = 0\n",
        "for i in range(662):\n",
        "  if labels[i] == predictions[i]:\n",
        "    j+=1\n",
        "print(\"%d predicted values matches the label out of total %d training set values.\" %((j,len(test_dataset))))"
      ],
      "metadata": {
        "id": "vbdnS-43KEtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKnc1WZk9cIk"
      },
      "source": [
        "### Instructions for preparing Kaggle competition predictions\n",
        "\n",
        "\n",
        "* Get the predictions using trained model and prepare a csv file\n",
        "    * DeepNet model gives output for each class, consider the maximum value among all classes as prediction using `np.argmax`.\n",
        "\n",
        "* Predictions (csv) file should contain 2 columns as Sample_Submission.csv\n",
        "  - First column is the Test_Id which is considered as index\n",
        "  - Second column is prediction in decoded form (for eg. Positive, Negative etc...)."
      ]
    }
  ]
}